{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842dae4e",
   "metadata": {},
   "source": [
    "# Shapley calculations: Example II (multiple cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae8909",
   "metadata": {},
   "source": [
    "In this example, we have previously created a classifier. We have the data used to create this classifier, and now we want to compute SHAP explainability scores for this classifier using multiple CPU cores (to speed things up a bit). Time should scale linearly with the number of cores available. Because the model has to be pushed to each core, it's advisable to use as slim of a model as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16951076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simba.mixins.train_model_mixin import TrainModelMixin\n",
    "from simba.mixins.config_reader import ConfigReader\n",
    "from simba.utils.read_write import read_config_file, read_pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd92c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITIONS\n",
    "CONFIG_PATH = r\"C:\\troubleshooting\\mitra\\project_folder\\project_config.ini\"\n",
    "CLASSIFIER_PATH = r\"C:\\troubleshooting\\mitra\\models\\generated_models\\grooming.sav\"\n",
    "CLASSIFIER_NAME = 'grooming'\n",
    "COUNT_PRESENT = 250\n",
    "COUNT_ABSENT = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c206a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN THE CONFIG AND THE CLASSIFIER\n",
    "config = read_config_file(config_path=CONFIG_PATH)\n",
    "config_object = ConfigReader(config_path=CONFIG_PATH)\n",
    "clf = read_pickle(data_path=CLASSIFIER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d453a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 544.10988MB / 0.54411GB\n"
     ]
    }
   ],
   "source": [
    "# READ IN THE DATA \n",
    "\n",
    "#Read in the path to all files inside the project_folder/csv/targets_inserted directory\n",
    "file_paths = glob.glob(config_object.targets_folder + '/*' + config_object.file_type)\n",
    "\n",
    "#Reads in the data held in all files in ``file_paths`` defined above\n",
    "data, _ = TrainModelMixin().read_all_files_in_folder_mp(file_paths=file_paths, file_type=config.get('General settings', 'workflow_file_type').strip())\n",
    "\n",
    "#We find all behavior annotations that are NOT the targets. I.e., if SHAP values for Attack is going to be calculated, bit we need to find which other annotations exist in the data e.g., Escape and Defensive.\n",
    "non_target_annotations = TrainModelMixin().read_in_all_model_names_to_remove(config=config, model_cnt=config_object.clf_cnt, clf_name=CLASSIFIER_NAME)\n",
    "\n",
    "# We remove the body-part coordinate columns and the annotations which are not the target from the data  \n",
    "data = data.drop(non_target_annotations + config_object.bp_headers, axis=1)\n",
    "\n",
    "# We place the target data in its own variable\n",
    "target_df = data.pop(CLASSIFIER_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edbe5dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 500 SHAP values (MULTI-CORE BATCH SIZE: 100, FOLLOW PROGRESS IN OS TERMINAL)...\n",
      "Concatenating multi-processed SHAP data (batch 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 500 out of 500 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating multi-processed SHAP data (batch 2/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 500 out of 500 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating multi-processed SHAP data (batch 3/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 500 out of 500 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating multi-processed SHAP data (batch 4/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 500 out of 500 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating multi-processed SHAP data (batch 5/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 500 out of 500 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMBA COMPLETE: SHAP calculations complete (elapsed time: 231.2415s) \tcomplete\n",
      "SIMBA WARNING: ShapWarning: SHAP visualizations/aggregate stats skipped (only viable for projects with two animals and default 7 or 8 body-parts per animal) ... \twarning\n"
     ]
    }
   ],
   "source": [
    "TrainModelMixin().create_shap_log_mp(rf_clf=clf,\n",
    "                                     x=data,\n",
    "                                     y=target_df,\n",
    "                                     x_names=list(data.columns),\n",
    "                                     clf_name=CLASSIFIER_NAME,\n",
    "                                     cnt_present=COUNT_PRESENT,\n",
    "                                     cnt_absent=COUNT_ABSENT,\n",
    "                                     core_cnt=2,\n",
    "                                     chunk_size=100,\n",
    "                                     verbose=True,\n",
    "                                     save_dir=config_object.logs_path,\n",
    "                                     save_file_suffix=1,\n",
    "                                     plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed794ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simba",
   "language": "python",
   "name": "simba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

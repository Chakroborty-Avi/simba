<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Scenario 1 walkthrough &mdash; SimBA 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/simba_theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css" />

  
    <link rel="shortcut icon" href="../_static/readthedocs_logo.png"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Scenario 2 walkthrough" href="scenario_2.html" />
    <link rel="prev" title="Walkthroughs" href="../walkthroughs.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            SimBA
              <img src="../_static/readthedocs_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">API REFERENCE:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NOTEBOOKS:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notebooks.html">Notebooks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">USER GUIDE / TUTORIALS:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">WALKTHROUGHS:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../walkthroughs.html">Walkthroughs</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Scenario 1 walkthrough</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hypothetical-data-set">Hypothetical data set:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scenario-1-from-scratch"><strong>Scenario 1</strong>: From scratch…</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#pipeline-breakdown">Pipeline breakdown:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#part-1-create-a-new-project">Part 1: Create a new project</a></li>
<li class="toctree-l4"><a class="reference internal" href="#part-2-load-project">Part 2: Load project</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Part 1: Create a new project</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#step-1-generate-project-config">Step 1: Generate Project Config</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step-2-import-videos-into-project-folder">Step 2: Import videos into project folder</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step-3-import-dlc-tracking-data">Step 3: Import DLC Tracking Data</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id2">Part 2: Load project</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#step-1-load-project-config">Step 1: Load Project Config</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step-2-optional-step-import-more-dlc-tracking-data-or-videos">Step 2 (Optional step) : Import more DLC Tracking Data or videos</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step-3-set-video-parameters">Step 3: Set video parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step-4-outlier-correction">Step 4: Outlier Correction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step-5-extract-features">Step 5: Extract Features</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step-6-label-behavior-i-e-create-annotations-for-predictive-classifiers">Step 6: Label Behavior (i.e, create annotations for predictive classifiers)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step-7-train-machine-model">Step 7: Train Machine Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step-8-evaluating-the-model-on-new-out-of-sample-data">Step 8. Evaluating the model on new (out-of-sample) data.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="scenario_2.html">Scenario 2 walkthrough</a></li>
<li class="toctree-l2"><a class="reference internal" href="scenario_3.html">Scenario 3 walkthrough</a></li>
<li class="toctree-l2"><a class="reference internal" href="scenario_3.html#hypothetical-experiment"><strong>Hypothetical Experiment</strong>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="scenario_3.html#scenario-3-updating-a-classifier-with-further-annotated-data"><strong>Scenario 3</strong>: Updating a classifier with further annotated data.</a></li>
<li class="toctree-l2"><a class="reference internal" href="scenario_4.html">Scenario 3 walkthrough</a></li>
<li class="toctree-l2"><a class="reference internal" href="scenario_4.html#hypothetical-experiment"><strong>Hypothetical Experiment</strong>:</a></li>
<li class="toctree-l2"><a class="reference internal" href="scenario_4.html#scenario-4-analyzing-and-adding-new-experimental-data-to-a-previously-started-project"><strong>Scenario 4</strong>: Analyzing and adding new Experimental data to a previously started project.</a></li>
<li class="toctree-l2"><a class="reference internal" href="scenario_4.html#part-6-post-classification-validation-detecting-false-positives">PART 6: Post-classification Validation (detecting false-positives)</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">LABELLING TUTORIALS:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../labelling.html">Labelling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">Friendly Asked Questions (FAQ)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DOCS:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../docs/workflow.html">SimBA basic workflow</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ABOUT:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html#who-writes-this-stuff">Who writes this stuff??</a></li>
<li class="toctree-l1"><a class="reference internal" href="../links.html">Links</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SimBA</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../walkthroughs.html">Walkthroughs</a></li>
      <li class="breadcrumb-item active">Scenario 1 walkthrough</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials_rst/scenario_1.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="scenario-1-walkthrough">
<h1>Scenario 1 walkthrough<a class="headerlink" href="#scenario-1-walkthrough" title="Permalink to this heading"></a></h1>
<p>To faciliate the initial use of SimBA, we provide several use scenarios.
We have created these scenarios around a hypothetical experiment that
take a user from initial use (completely new start) all the way through
analyzing a complete experiment and then adding additional experimental
datasets to an initial project.</p>
<p>All scenarios assume that the videos have been
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial_process_videos.md">pre-processed</a>
and that DeepLabCut CSV pose-estimation tracking files have been
created.</p>
<section id="hypothetical-data-set">
<h2>Hypothetical data set:<a class="headerlink" href="#hypothetical-data-set" title="Permalink to this heading"></a></h2>
<p>Three days of resident-intruder testing between aggressive CD-1 mice and
subordinante C57 intruders. Each day of testing has 10 pairs of mice,
for a total of 30 videos recorded across 3 days. Recordings are 3
minutes in duration, in color, at 30fps.</p>
<p>Also, so that we do not overfit the predictive classifiers to the
experimental data, a different set of pilot videos have been recorded of
resident-inturder pairings using identical video acquisition parameters.
A total of 20 pilot videos were recorded.</p>
</section>
<section id="scenario-1-from-scratch">
<h2><strong>Scenario 1</strong>: From scratch…<a class="headerlink" href="#scenario-1-from-scratch" title="Permalink to this heading"></a></h2>
<p>In this series of Scenarios, you have the above pilot videos and DLC
tracking data, the experimental videos and DLC tracking data, and SimBA.
All videos have been pre-processed and DLC tracked. You now would like
to generate a new predictive classifier for “Behavior that Will Get a
Nature Paper (Behavior BtWGaNP)”. In this first Scenario, we will use
the pilot data to generate the classifier for behavior BtWGaNP. In later
scenario tutorials, we will use this predictive classifier to classify
BtWGaNP behaviours for Day 1 of the experiment (<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Scenario2.md">Scenario
2</a>,
add behavioural data with the goal of improving the behavioral
classifier (<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Scenario3.md">Scenario
3</a>,
and use the predictive classifier to classify behaviour BtWGaNP for Day
2 of the experiment (<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Scenario4.md">Scenario
4</a>).</p>
<section id="pipeline-breakdown">
<h3>Pipeline breakdown:<a class="headerlink" href="#pipeline-breakdown" title="Permalink to this heading"></a></h3>
<p>For processing datasets, the pipeline is split into a few sections.
These sections are listed below along with their corresponding
functions:</p>
<a class="reference internal image-reference" href="tutorials_rst/img/scenario_1/pipeline_1.jpg"><img alt="tutorials_rst/img/scenario_1/pipeline_1.jpg" class="align-center" src="tutorials_rst/img/scenario_1/pipeline_1.jpg" style="width: 1200px;" /></a>
</section>
<section id="part-1-create-a-new-project">
<h3>Part 1: Create a new project<a class="headerlink" href="#part-1-create-a-new-project" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-1-generate-project-config">Generate project
config</a>
(create new classifiers)</p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-2-import-videos-into-project-folder">Import videos into project
folder</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-3-import-dlc-tracking-data">Import DLC Tracking
Data</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-4-extract-frames-into-project-folder">Extract Frames into project
folder</a></p></li>
</ul>
</section>
<section id="part-2-load-project">
<h3>Part 2: Load project<a class="headerlink" href="#part-2-load-project" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-1-load-project-config">Load the
project.ini</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-3-set-video-parameters">Set video
parameters</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-4-outlier-correction">Outlier
correction</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-5-extract-features">Extract
Features</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-6-label-behavior">Label
Behavior</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-7-train-machine-model">Train Machine
Model</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-8-run-machine-model">Run Machine
Model</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-9-analyze-machine-results">Analyze Machine
Results</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-11-visualizations">Visualization</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#gantt-plot">Plot
Graphs</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-12-merge-frames">Merge
Frames</a></p></li>
<li><p><a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#step-13-create-videos">Create
Video</a></p></li>
</ul>
</section>
</section>
<section id="id1">
<h2>Part 1: Create a new project<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h2>
<p>This section describes how to create a new project for behavioral
classifier predictions.</p>
<a class="reference internal image-reference" href="tutorials_rst/img/scenario_1/create_project.png"><img alt="tutorials_rst/img/scenario_1/create_project.png" class="align-center" src="tutorials_rst/img/scenario_1/create_project.png" style="width: 1200px;" /></a>
<section id="step-1-generate-project-config">
<h3>Step 1: Generate Project Config<a class="headerlink" href="#step-1-generate-project-config" title="Permalink to this heading"></a></h3>
<p>In this step you create your main project folder, which will then
auto-populate with all the required sub-directories.</p>
<ol class="arabic simple">
<li><p>In the main SimBA window, click on <code class="docutils literal notranslate"><span class="pre">File</span></code> and and
<code class="docutils literal notranslate"><span class="pre">Create</span> <span class="pre">a</span> <span class="pre">new</span> <span class="pre">project</span></code>. The following windows will pop up.</p></li>
</ol>
<a class="reference internal image-reference" href="../_images/create_project_1.png"><img alt="../_images/create_project_1.png" class="align-center" src="../_images/create_project_1.png" style="width: 600px;" /></a>
<ol class="arabic simple" start="2">
<li><p>Navigate to the <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">Generate</span> <span class="pre">project</span> <span class="pre">config</span> <span class="pre">]</span></code> tab. Under <strong>General
Settings</strong>, specify a <code class="docutils literal notranslate"><span class="pre">Project</span> <span class="pre">Path</span></code> which is the directory that
will contain your main project folder.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Project</span> <span class="pre">Name</span></code> is the name of your project. *Keep in mind that the
project name cannot contain spaces. We suggest to instead use
underscore “_“*</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">SML</span> <span class="pre">Settings</span></code> sub-menu, put in the number of predictive
classifiers that you wish to create. For an example, in Scenario 1 we
would like to create a single classifier. We will enter the number 1.
Note that in the real world you would probably want to create
multiple classifiers concurrently as this would decrease the number
of times a video would need to be manually annotated. For simplicity,
we will here create a single classifier. <em>Note: If you are using
SimBA only for region of intrest (ROI) analysis, and do not wish to
create any classifiers, enter ``1`` in this entry box.</em></p></li>
<li><p>Click a single time, and it creates a row as shown in the following
image. In each entry box, fill in the name of the behavior (BtWGaNP)
that you want to classify. If you click too many times, as long as
you leave the extra boxes empty, all is well. <em>Note: If you are using
SimBA only for region of intrest (ROI) analysis, and do not wish to
create any classifiers, enter any name in the the single entry box.</em></p></li>
</ol>
<a class="reference internal image-reference" href="../_images/create_project_2.png"><img alt="../_images/create_project_2.png" class="align-center" src="../_images/create_project_2.png" style="width: 600px;" /></a>
<ol class="arabic simple" start="6">
<li><p>The sub-menu <code class="docutils literal notranslate"><span class="pre">Animal</span> <span class="pre">Settings</span> <span class="pre">-</span> <span class="pre">#</span> <span class="pre">config</span></code> is used to specify the
number of animals and body parts that that the pose estimation
tracking data contains. The default for <strong>SimBA</strong> is 2 animals and 16
body parts (<code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">animals,</span> <span class="pre">16bp</span></code>). There are a few other - <strong>yet not
validaded</strong> - options, accessible in the dropdown menu. This
selection is the annotation configuration you should have previously
used when labelling images in DeepLabCut or DeepPoseKit - see the
tutorial for <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Tutorial_DLC.md#pose-estimation-body-part-labelling">Pose estimation body-part
labelling</a>
for more information.</p></li>
</ol>
<p>The second drop-down in the <code class="docutils literal notranslate"><span class="pre">Animal</span> <span class="pre">Settings</span></code> menu is the
<code class="docutils literal notranslate"><span class="pre">Type</span> <span class="pre">of</span> <span class="pre">Tracking</span></code> menu. In the current scenario we will leave this at
the default <code class="docutils literal notranslate"><span class="pre">Classical</span> <span class="pre">tracking</span></code>. <code class="docutils literal notranslate"><span class="pre">Classical</span> <span class="pre">tracking</span></code> is used when
your videos contain one animal, or two animals that are clearly (by eye)
discriminable, such as a white and a black coated animal. A second
option in the <code class="docutils literal notranslate"><span class="pre">Type</span> <span class="pre">of</span> <span class="pre">Tracking</span></code> dropdown menu is <code class="docutils literal notranslate"><span class="pre">Multi</span> <span class="pre">tracking</span></code>.
Select the <code class="docutils literal notranslate"><span class="pre">Multi</span> <span class="pre">tracking</span></code> option from the <code class="docutils literal notranslate"><span class="pre">Type</span> <span class="pre">of</span> <span class="pre">Tracking</span></code>
dropdown menu <em>only when</em> the animals in your protocol are not
discriminabl by eye and you have used newer pose-estimation tools to
perform your animal tracking. These newer tools include multi-animal
DeepLabCut <a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/releases/tag/v2.2b5">DLC version
&gt;=2.2b5</a>
and <a class="reference external" href="https://sleap.ai/">SLEAP</a>. In this tutorial we will use
<code class="docutils literal notranslate"><span class="pre">Classical</span> <span class="pre">tracking</span></code>. If you have multi-animal tracking data from
<a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/releases/tag/v2.2b5">DeepLabCut
&gt;=2.2b5</a>
or <a class="reference external" href="https://sleap.ai/">SLEAP</a>, then head over to the seperate
tutorial on <em>Import multi-animal data</em> to learn how to import this data
into SimBA. Come back to this tutorial after you have completed the
<em>Import multi-animal data</em> tutorial and your multi-animal data has been
imported into SimBA.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to use a different body-part configuration that is
not shown in the drop-down menu - go tho the tutorial for creating
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/simba_JJ_branch/docs/Pose_config.md">user-defined
pose-configurations</a>
in SimBA.</p>
</div>
<ol class="arabic simple" start="7">
<li><p>Click on <code class="docutils literal notranslate"><span class="pre">Generate</span> <span class="pre">Project</span> <span class="pre">Config</span></code> to generate your project. The
project folder will be located in the specified <code class="docutils literal notranslate"><span class="pre">Project</span> <span class="pre">Path</span></code>. The
Project will remain open, and the main console will report that the
project has been created.</p></li>
</ol>
</section>
<section id="step-2-import-videos-into-project-folder">
<h3>Step 2: Import videos into project folder<a class="headerlink" href="#step-2-import-videos-into-project-folder" title="Permalink to this heading"></a></h3>
<p>In general, you can choose to import either one or multiple videos. The
imported videos are used for visualizing predictions, standardizing
distances across videos by calculating metric distances from pixel
distances, and annotating behaviours for supervised machine learning
classifiers.</p>
<p>In this current Scenario 1, we now want to import the 20 pilot videos
that we will use to train the classifier for Behavior BtWGaNP.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are several considerations when selecting the number of
videos for training classifers. Most importantly, the number of
videos is not as important as the total number of behavioral events
present within the videos. Using something that is frequent, such as
attacks, may only require a handfull of videos; using something that
is less frrequent, such as lateral threat displays, will require more
videos for the same number of events. Further, the expression of
these behaviors should be representative of the overall experimental
data (i.e., attack behaviors exhibited should generalize to how
attacks normally look, and not only be extremely robust or extemely
weak). Lastly, you need enough videos that some can be left over to
validate the generated predicitive classifiers against. We do not
want to test the classifiers on videos that have been used to
generate the classifiers. <strong>Here we import 20 videos, having
identified that 19 videos contain enough events of Behavior BtWGaNP
to get an acceptable sample, and that the remaining video will be
used for validation</strong>. As a sanity check, we can also create rendered
visualizations of the data that is processed in the subsequent days
and phases of Scenarios
<a class="reference external" href="https://github.com/sgoldenlab/simba/tree/master/docs">2-4</a>. We
will also evaluate the accuracy of the predictive classifier using
various <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Scenario1.md#step-7-train-machine-model">evaluation
tools</a>
built into SimBA.</p>
</div>
<a class="reference internal image-reference" href="../_images/import_videos.png"><img alt="../_images/import_videos.png" class="align-center" src="../_images/import_videos.png" style="width: 600px;" /></a>
<section id="to-import-multiple-videos">
<h4>To import multiple videos<a class="headerlink" href="#to-import-multiple-videos" title="Permalink to this heading"></a></h4>
<ol class="arabic simple">
<li><p>Navigate to the <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">Import</span> <span class="pre">videos</span> <span class="pre">into</span> <span class="pre">project</span> <span class="pre">folder</span> <span class="pre">]</span></code> tab.</p></li>
<li><p>Under the <code class="docutils literal notranslate"><span class="pre">Import</span> <span class="pre">multiple</span> <span class="pre">videos</span></code> heading, click on
<code class="docutils literal notranslate"><span class="pre">Browse</span> <span class="pre">Folder</span></code> to select a folder that contains <strong>all the pilot
videos</strong> to import into your project.</p></li>
<li><p>Enter the file type of your videos. (e.g., <em>mp4</em>, <em>avi</em>, <em>mov</em>, etc)
in the <code class="docutils literal notranslate"><span class="pre">Video</span> <span class="pre">type</span></code> entry box.</p></li>
<li><p>Click on <code class="docutils literal notranslate"><span class="pre">Import</span> <span class="pre">multiple</span> <span class="pre">videos</span></code>. &gt;<em>Note</em>: If you have a lot of
videos, or exceptionally high resolution/frame rate/duration videos,
it might take a few minutes before all the videos are imported. The
main SimBA terminal window will report when the process is complete.</p></li>
</ol>
</section>
</section>
<section id="step-3-import-dlc-tracking-data">
<h3>Step 3: Import DLC Tracking Data<a class="headerlink" href="#step-3-import-dlc-tracking-data" title="Permalink to this heading"></a></h3>
<p>In this step, you will import your pose-estimation tracking data from
DeepLabCut in CSV file format. For the pilot videos, this means that you
should have 20 individual CSV files corresponding to each of the
individual videos.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DLC outputs CSV files with exceptionally long file names. We
have included a function that will automatically copy the DLC CSV
files, remove the excess DLC notations from the filename, and paste
the new CSV files within the SimBA project. The new CSV will now have
the same name as the corresponding video. The original DLC output CSV
will still be present in its original folder.</p>
</div>
<a class="reference internal image-reference" href="../_images/import_data_1.png"><img alt="../_images/import_data_1.png" class="align-center" src="../_images/import_data_1.png" style="width: 600px;" /></a>
<section id="to-import-multiple-dlc-csv-files">
<h4>To import multiple DLC csv files<a class="headerlink" href="#to-import-multiple-dlc-csv-files" title="Permalink to this heading"></a></h4>
<ol class="arabic simple">
<li><p>Navigate to the <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">Import</span> <span class="pre">tracking</span> <span class="pre">data</span> <span class="pre">]</span></code> tab. The first dropdown
menu specify your file-type. In this tutorial we are importing CSV
files from DLC and we will leave this at the default (<strong>CSV
(DLC/DeepPoseKit)</strong>). If you have pose-estimation data in alternative
file-formats, for example from multi-animal DLC or SLEAP, then please
see the seperate <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Multi_animal_pose.md">SimBA multi-animal
tutorial</a>
for how to import this data.</p></li>
<li><p>Under the <code class="docutils literal notranslate"><span class="pre">Interpolate</span> <span class="pre">missing</span> <span class="pre">pose-estimation</span> <span class="pre">data</span></code> heading,
select how SimBA should handle missing animals in your
pose-estimation input files. For best classifier performance, it is
important that the animals are present in all the videos throughout
the recording. We recommend that you use the <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Tutorial_tools.md#shorten-videos">SimBA video clipping
tools</a>
to remove time-segments of the video where the animals are absent
from the recorded arena. However - if clipping the videos is not an
option - then we can tell SimBA to interpolate the missing values.</p></li>
</ol>
<p>You can tell SimBA to <em>only</em> interpolate frames where entire animals are
missing (the <code class="docutils literal notranslate"><span class="pre">Animal(s):</span> <span class="pre">Nearest</span></code>, <code class="docutils literal notranslate"><span class="pre">Animals(s):</span> <span class="pre">Linear</span></code> and
<code class="docutils literal notranslate"><span class="pre">Animal(s)</span> <span class="pre">Quadratic</span></code> options in the dropdown menu shown below) or you
can tell SimBA to interpolate every occurance of a missing body-part
(the <code class="docutils literal notranslate"><span class="pre">Body-parts:</span> <span class="pre">Nearest</span></code>, <code class="docutils literal notranslate"><span class="pre">Body-parts:</span> <span class="pre">Linear</span></code> and
<code class="docutils literal notranslate"><span class="pre">Body-parts:</span> <span class="pre">Quadratic</span></code> options in the dropdown menu shown below). For
both option, SimBA offers three methods of interpolation - linear,
nearest, and quadratic. Figures that show the differences between
interpolation methods can be found
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/images/Interpolation_comparison.png">HERE</a>
or
<a class="reference external" href="https://gist.github.com/gyassine/b47b90e8de935cc06ef856401f7582b0">HERE</a>.</p>
<a class="reference internal image-reference" href="../_images/interpolation_1.png"><img alt="../_images/interpolation_1.png" class="align-center" src="../_images/interpolation_1.png" style="width: 600px;" /></a>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We recommend that you make sure that the animals are not
absent from the video recording and that you pre-process the videos
by removing video segments where the animals are not present, before
performing pose-estimation and importing the data into SimBA.
Interpolations will result in feeding inaccurate data into SimBA and
should be avoided where possible.</p>
</div>
<ol class="arabic simple" start="3">
<li><p>Occasionally, the data coming out of pose-estimation packages is
“jittery”, and body-part predictions show small, but unrealistic,
“jumps” between sequential frames. “Smoothing” is an optional
pre-processing step where the pose-estimation predictions, in any
given frame, are averaged with the predictions in the preceding and
proceeding frames. This pre-processing step can serve to generate
more realistic and truthful tracking predictions. For a
<em>before-and-after</em> video example of expected output of “smoothing”,
check out <a class="reference external" href="https://youtu.be/d9-Bi4_HyfQ">THIS VIDEO</a>.</p></li>
</ol>
<p>SimBA allows the option to smooth the post-estimation data using a
<a class="reference external" href="https://towardsdatascience.com/gaussian-smoothing-in-time-series-data-c6801f8a4dc">Gaussian</a>
or a <a class="reference external" href="https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter">Savitzky
Golay</a>
kernel when importing the data. To perform Gaussian smoothing, select
<code class="docutils literal notranslate"><span class="pre">Gaussian</span></code> in the “Smoothing data” dropdown menu. To perform
Savitzky-Golay smoothing, select <code class="docutils literal notranslate"><span class="pre">Savitzky</span> <span class="pre">Golay</span></code> in the “Smoothing
data” dropdown menu. Once selected in the dropdown, an entry box titled
<code class="docutils literal notranslate"><span class="pre">Time</span> <span class="pre">window</span> <span class="pre">(ms)</span></code> will be displayed next to the drop-down menu, like
in the image below. In this entry box, we specify a time window in
milliseconds (e.g., 200). This time window will be used by SimBA in the
background to identify the preceding and proceeding frames, the
pose-estimation predictions in those frames, and how much influence
those predictions should have when correcting the current prediction. If
you are unsure what value to use, I recommend to begin with a value
somewhere between <code class="docutils literal notranslate"><span class="pre">100-300</span></code>. In this tutorial we will skip smoothing
of your pose-estimation data, and select <code class="docutils literal notranslate"><span class="pre">None</span></code> in the <code class="docutils literal notranslate"><span class="pre">Smoothing</span></code>
drop-down menu.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>After performing <code class="docutils literal notranslate"><span class="pre">interpolation</span></code> and/or <code class="docutils literal notranslate"><span class="pre">smoothing</span></code>, we
may want to visualize the results before proceeding with the rest of
the analysis pipeline; to confirm the interpolation and/or smoothing
improves the pose-estimation tracking. To visualize the results, I
recommend to use the <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Tutorial_tools.md#visualize-pose-estimation-in-folder">Visualize pose estimation in
folder</a>
function that can be accessed throught the
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Tutorial_tools.md#visualize-pose-estimation-in-folder">Tools</a>
menu in the main SimBA terminal.</p>
</div>
<a class="reference internal image-reference" href="../_images/smoothing_1.png"><img alt="../_images/smoothing_1.png" class="align-center" src="../_images/smoothing_1.png" style="width: 600px;" /></a>
<ol class="arabic simple" start="4">
<li><p>Under the <code class="docutils literal notranslate"><span class="pre">Import</span> <span class="pre">multiple</span> <span class="pre">csv</span> <span class="pre">files</span></code> heading, click on
<code class="docutils literal notranslate"><span class="pre">Browse</span> <span class="pre">Folder</span></code> to select the folder that contains the CSV files
that you wish to import into your project. Click on
<code class="docutils literal notranslate"><span class="pre">Import</span> <span class="pre">csv</span> <span class="pre">to</span> <span class="pre">project</span> <span class="pre">folder</span></code>.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>SimBA also provides the ability to import single videos and
their corresponding CSV files. For more information, click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial.md#part-1-create-a-new-project-1">here</a>.
This method is not used in Scenario 1.</p>
</div>
</section>
</section>
</section>
<section id="id2">
<h2>Part 2: Load project<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h2>
<p>In Part 1, we created a project. To continue working with this project,
we <strong>must</strong> load it. This section describes how to load and work with
SimBA projects.</p>
<section id="step-1-load-project-config">
<h3>Step 1: Load Project Config<a class="headerlink" href="#step-1-load-project-config" title="Permalink to this heading"></a></h3>
<p>In this step you will load the <em>project_config.ini</em> file that was
created. &gt; <em>Note:</em> A project_config.ini should <strong>always</strong> be loaded
before any other process.</p>
<ol class="arabic simple">
<li><p>In the main SimBA window, click on <code class="docutils literal notranslate"><span class="pre">File</span></code> and <code class="docutils literal notranslate"><span class="pre">Load</span> <span class="pre">project</span></code>. The
following windows will pop up.</p></li>
</ol>
<a class="reference internal image-reference" href="../_images/load_project_1.png"><img alt="../_images/load_project_1.png" class="align-center" src="../_images/load_project_1.png" style="width: 800px;" /></a>
<ol class="arabic simple" start="2">
<li><p>Click on <code class="docutils literal notranslate"><span class="pre">Browse</span> <span class="pre">File</span></code>. Then, navigate to the directory that you
created your project in and click on your <em>project folder</em>. Locate
the <em>project_config.ini</em> file and select it. Once this step is
completed, the <code class="docutils literal notranslate"><span class="pre">File</span> <span class="pre">Selected</span></code> entry box should look like the
following, and you should no longer see the text <em>No file selected</em>:</p></li>
</ol>
<a class="reference internal image-reference" href="../_images/load_project_2.png"><img alt="../_images/load_project_2.png" class="align-center" src="../_images/load_project_2.png" style="width: 800px;" /></a>
<p>In this image, you can see the <code class="docutils literal notranslate"><span class="pre">Desktop</span></code> is my selected working
directory, <code class="docutils literal notranslate"><span class="pre">tutorial</span></code> is my project name, and the last two sections of
the folder path is always going to be
<code class="docutils literal notranslate"><span class="pre">project_folder/project_config.ini</span></code>.</p>
</section>
<section id="step-2-optional-step-import-more-dlc-tracking-data-or-videos">
<h3>Step 2 (Optional step) : Import more DLC Tracking Data or videos<a class="headerlink" href="#step-2-optional-step-import-more-dlc-tracking-data-or-videos" title="Permalink to this heading"></a></h3>
<p>In this step, you can choose to import more pose estimation data and/or
more videos. You can only archive analysed files, define new
classifiers, and remove previously defined classifiers. If this isn’t
relevant then you can skip this step. This is however relevant for
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Scenario4.md#part-2-load-the-project-and-import-your-new-data">Scenario
4</a>
and you can read more about these options in the <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Scenario4.md#part-2-load-the-project-and-import-your-new-data">Scenario 4
tutorial</a>.
However, this is not relevant for the current Scenario 1, so please skip
to Step 3 (but remain aware you can do this if needed later).</p>
<a class="reference internal image-reference" href="../_images/create_project_3.png"><img alt="../_images/create_project_3.png" class="align-center" src="../_images/create_project_3.png" style="width: 1000px;" /></a>
<ol class="arabic simple">
<li><p>Click on the <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">Further</span> <span class="pre">imports</span> <span class="pre">(data/video/frames)</span> <span class="pre">]</span></code> tab. From
here you can import more data or videos into the project folder. The
imported .csv files will be placed in the
<code class="docutils literal notranslate"><span class="pre">project_folder/csv/input</span></code> directory, and the imported videos will
be placed in the <code class="docutils literal notranslate"><span class="pre">project_folder/videos</span></code> directory.</p></li>
<li><p>Once the videos are imported, you can extract frames from the
additional videos by clicking on <code class="docutils literal notranslate"><span class="pre">Extract</span> <span class="pre">frames</span></code> under the
<strong>Extract further frames into project folder</strong> heading.</p></li>
<li><p>If you already have existing frames of the videos in the project
folder, you can import the folder that contains the frames into the
project. Under the <strong>Import frame folders</strong> heading, click on
<code class="docutils literal notranslate"><span class="pre">Browse</span> <span class="pre">Folder</span></code> to choose the folder thar contains the frames, and
click on <code class="docutils literal notranslate"><span class="pre">Import</span> <span class="pre">frames</span></code>. The frames will be imported into the
<code class="docutils literal notranslate"><span class="pre">project_folder/frames/input</span></code> folder.</p></li>
<li><p>If you would like to <em>add</em> a new classifier to the current project,
type the name of the new classifier in the <code class="docutils literal notranslate"><span class="pre">Classifier</span></code> entry box
in the <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">new</span> <span class="pre">classifier(s)</span></code> sub-menu.</p></li>
<li><p>If you would like to <em>remove</em> a previously defined classifier from
the current project, click on <code class="docutils literal notranslate"><span class="pre">Choose</span> <span class="pre">a</span> <span class="pre">classifier</span> <span class="pre">to</span> <span class="pre">remove</span></code> in
the <code class="docutils literal notranslate"><span class="pre">Remove</span> <span class="pre">existing</span> <span class="pre">classifiers(s)</span></code> submenu. Once clicked, a
window will appear with a dropdown menu where you select the
classifier you wish to remove from the current project.</p></li>
<li><p>Once you have analyzed your videos, and/or used videos to create
classifiers in SimBA (see <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Scenario4.md#part-2-load-the-project-and-import-your-new-data">Scenario
4</a>),
you may want to archive your analyzed files to prevent them from
being analyzed by SimBA again. Use the <code class="docutils literal notranslate"><span class="pre">Archive</span> <span class="pre">processed</span> <span class="pre">files</span></code>
menu to enter a folder name for your analyzed files and click on
<code class="docutils literal notranslate"><span class="pre">Archive</span></code> to move your analyzed files into this folder.</p></li>
</ol>
</section>
<section id="step-3-set-video-parameters">
<h3>Step 3: Set video parameters<a class="headerlink" href="#step-3-set-video-parameters" title="Permalink to this heading"></a></h3>
<p>In this step, you can customize the meta parameters for each of your
videos (fps, resolution, metric distances) and provide additional custom
video information (Animal ID, group etc). This can be very helpful when
analyzing data later on. Are there any groups, conditions, days,
treatments, etc, that will make your analysis easier? Note that if youd
like to change any of these parameters, you can do so with the SimBA
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/tutorial_process_videos.md">video pre-processing
tools</a>.
Note that the video pre-processing should be performed prior to
analyzing the videos through DeepLabCut, SLEAP, or DeepPoseKit.</p>
<p>Importantly, in these menues, you also set the <strong>pixels per millimeter</strong>
for all of your videos. You will be using a tool that requires the known
distance between two points (e.g., the cage width or the cage height) in
order to calculate <strong>pixels per millimeter</strong>. We calculate this measure
in each video so that we can standardize variables in metric units, and
no longer become bound by working with pixel location data. This means
that it does not matter if your camera moved slighly across different
video recording sessions.</p>
<ol class="arabic simple">
<li><p>Under <strong>Set video parameters (distances,resolution,etc.)</strong>, the entry
box named <code class="docutils literal notranslate"><span class="pre">Distance</span> <span class="pre">in</span> <span class="pre">mm</span></code> is the known distance between two points
in the videos in <strong>millimeters</strong>. If the known distance is the same
in all the videos in the project, then enter the millimeter value
<em>(e.g,: 245)</em> and click on
<code class="docutils literal notranslate"><span class="pre">Auto</span> <span class="pre">populate</span> <span class="pre">Distance</span> <span class="pre">in</span> <span class="pre">mm</span> <span class="pre">in</span> <span class="pre">tables</span></code>. This will auto-populate
the table in the next step (see below). If you leave the
<code class="docutils literal notranslate"><span class="pre">Distance</span> <span class="pre">in</span> <span class="pre">mm</span></code> entry box empty, the known distance will default
to zero and you will fill in the value for each video individually.</p></li>
<li><p>Click on <code class="docutils literal notranslate"><span class="pre">Set</span> <span class="pre">Video</span> <span class="pre">Parameters</span></code> and the following windows will pop
up.</p></li>
</ol>
<a class="reference internal image-reference" href="../_images/video_info_1.png"><img alt="../_images/video_info_1.png" class="align-center" src="../_images/video_info_1.png" style="width: 1000px;" /></a>
<ol class="arabic simple" start="3">
<li><p>In the above example we imported the 20 pilot videos and their names
are listed the left most <code class="docutils literal notranslate"><span class="pre">Video</span></code> column. SimbA auto-populated the
known distance to 245 millimeter in the previous step, and this is
now displayed in the <code class="docutils literal notranslate"><span class="pre">Distance</span> <span class="pre">in</span> <span class="pre">mm</span></code> column.</p></li>
<li><p>You can click on the values in the entry boxes and change them until
you are satisfied. By default, the entry boxes are populated with the
meta data from the video files.</p></li>
<li><p>Regardless if you updated the values in the table or not, click on
<code class="docutils literal notranslate"><span class="pre">Update</span> <span class="pre">distance_in_mm</span></code> at the top of the window: this will save
the information displayed in the table into a CSV file. The CSV file
is saved as <em>video_info.csv</em> and is stored in the
<code class="docutils literal notranslate"><span class="pre">project_folder\logs</span></code> folder. After clicking on the
<code class="docutils literal notranslate"><span class="pre">Update</span> <span class="pre">distance_in_mm</span></code> button, you can proceed to the next step.</p></li>
<li><p>Next, to get the <code class="docutils literal notranslate"><span class="pre">Pixels/mm</span></code> for the first video, click on
<code class="docutils literal notranslate"><span class="pre">Video1</span></code> and the following window will pop up. The window that pops
up displays the first frame of <code class="docutils literal notranslate"><span class="pre">Video1</span></code>.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the frame is displayed, it may not be shown at the
correct aspect ratio. To fix this, drag the window corner to the
correct aspect ratio.</p>
</div>
<a class="reference internal image-reference" href="../_images/get_coordinates_1.png"><img alt="../_images/get_coordinates_1.png" class="align-center" src="../_images/get_coordinates_1.png" style="width: 800px;" /></a>
<ol class="arabic simple" start="7">
<li><p>Now, double <strong>left</strong> click to select two points that defines the
known distance in real life. In this case, we know that the two
<strong>pink connected dots</strong> represent a distance of 245 millimeter in
real life.</p></li>
</ol>
<a class="reference internal image-reference" href="../_images/get_coordinates_2.png"><img alt="../_images/get_coordinates_2.png" class="align-center" src="../_images/get_coordinates_2.png" style="width: 800px;" /></a>
<ol class="arabic simple" start="8">
<li><p>If you misplaced one or both of the dots, you can double click on
either of the dots to place them somewhere else in the image. Once
you are done, hit <code class="docutils literal notranslate"><span class="pre">Esc</span></code>.</p></li>
</ol>
<a class="reference internal image-reference" href="../_images/get_coordinates_3.gif"><img alt="../_images/get_coordinates_3.gif" class="align-center" src="../_images/get_coordinates_3.gif" style="width: 800px;" /></a>
<ol class="arabic simple" start="9">
<li><p>If every step is done correctly, the <code class="docutils literal notranslate"><span class="pre">Pixels/mm</span></code> column in the
table should populate with the number of pixels that represent one
millimeter,</p></li>
</ol>
<a class="reference internal image-reference" href="../_images/video_info_2.png"><img alt="../_images/video_info_2.png" class="align-center" src="../_images/video_info_2.png" style="width: 800px;" /></a>
<ol class="arabic simple" start="10">
<li><p>Repeat the steps for every video in the table, and once it is done,
click on <code class="docutils literal notranslate"><span class="pre">Save</span> <span class="pre">Data</span></code>. This will update the CSV file named
<strong>video_info.csv</strong> in <code class="docutils literal notranslate"><span class="pre">/project_folder/log</span></code> directory that
contains your video meta data.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you have a a very large amount of videos then this process
(Step 10 above) can become time-consuming. If you are confident that
your camera moved <strong>minimally</strong> across recordings, and that the
<code class="docutils literal notranslate"><span class="pre">Pixels/mm</span></code> therefore is near-identical across videos, then SimBA
has the option of applying the <code class="docutils literal notranslate"><span class="pre">Pixels/mm</span></code> created for the first
video in the table, on all other videos in the table. To proceed with
this, click on the red <code class="docutils literal notranslate"><span class="pre">Duplicate</span> <span class="pre">index</span> <span class="pre">1</span> <span class="pre">pixel/mm</span> <span class="pre">(CAUTION!)</span></code>
button.</p>
</div>
<a class="reference internal image-reference" href="../_images/video_info_3.png"><img alt="../_images/video_info_3.png" class="align-center" src="../_images/video_info_3.png" style="width: 800px;" /></a>
<ol class="arabic simple" start="11">
<li><p>You can also chose to add further columns to the meta data file
(e.g., AnimalID or experimental group) by clicking on the
<code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">Column</span></code> button. This information will be saved in additional
columns to your <strong>video_info.csv</strong> file.</p></li>
</ol>
</section>
<section id="step-4-outlier-correction">
<h3>Step 4: Outlier Correction<a class="headerlink" href="#step-4-outlier-correction" title="Permalink to this heading"></a></h3>
<p>Outlier correction is used to correct gross tracking inaccuracies by
detecting outliers based on movements and locations of body parts in
relation to the animal body length. For more details, click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/misc/Outlier_settings.pdf">here</a>.
The annotation options are based upon the yaml.config settings, and we
suggest that defaults be kept. The only values that must be manually
entered in the Settings menu are the <em>Location Criterion</em> and the
<em>Movement Criterion</em>, explained below.</p>
<ol class="arabic simple">
<li><p>Click on <code class="docutils literal notranslate"><span class="pre">Settings</span></code> and the following window will pop up.</p></li>
</ol>
<a class="reference internal image-reference" href="../_images/outlier_correction_2.png"><img alt="../_images/outlier_correction_2.png" class="align-center" src="../_images/outlier_correction_2.png" style="width: 800px;" /></a>
<ol class="arabic simple" start="2">
<li><p>Select the body parts for Animal 1 and Animal 2 that you want to use
to calculate a reference value. The reference value will be the mean
or median Euclidian distance in millimeters between the two body
parts of the two animals in all frames. Again, we suggest you keep
the default settings.</p></li>
<li><p>Enter values for the <code class="docutils literal notranslate"><span class="pre">Movement</span> <span class="pre">criterion</span></code> and the
<code class="docutils literal notranslate"><span class="pre">Location</span> <span class="pre">criterion</span></code>.</p></li>
</ol>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Movement</span> <span class="pre">criterion</span></code>. A body part coordinate will be flagged and
corrected as a “movement outlier” if the body part moves the
<em>reference value multiplied by the criterion value</em> across two
sequential frames. The reference value is the mean or median length
of the animal between the selected two body parts. The criteron value
is a multiplier of the reference value. For example, the distance
between Nose_1 and Tail_base_1 is 100mm and the movement criterion is
0.5, any body part that moves 50mm across two sequential frames will
be corrected. <strong>We suggest a movement criterion value between 0.7 to
1.5</strong>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Location</span> <span class="pre">criterion</span></code>. A body part coordinate will be flagged and
correct as a “location outlier” if the distance between the body part
and at least two other body parts belonging to the same animal are
longer than the <em>reference value multiplied by the criterion value</em>
within a single frame. The reference value is the mean or median
length of the animal between the selected two body parts. The
criteron value is a multiplier of the reference value. For example,
the distance between Nose_1 and Tail_base_1 is 100mm and the location
criterion is 1.5, any body part located greater than 150 mm from two
other body parts will be corrected. <strong>We suggest a location criterion
value of 1.5 or greater</strong>.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Movement outliers are corrected prior to location outliers.
Specifically, (i) movement outliers are first corrected, and (ii)
location outliers are then corrected on the dataframe that contains
the movement corrected data. That means that there is likely to be
<em>fewer</em> location outliers than movement outliers corrected.</p>
</div>
<p>Body parts flagged as movement or location outliers will be re-placed in
their last reliable coordinate.</p>
<ol class="arabic simple" start="4">
<li><p>Chose to calculate the <em>median or mean</em> Euclidian distance in
millimeters between the two body parts at the bottom of the
<code class="docutils literal notranslate"><span class="pre">Settings</span></code> window and click on <code class="docutils literal notranslate"><span class="pre">Confirm</span> <span class="pre">Config</span></code>.</p></li>
<li><p>Click to run the outlier correction. You can follow the progress in
the main SimBA window. Once complete, two new CSV log files will
appear in the <code class="docutils literal notranslate"><span class="pre">/project_folder/log</span></code> folder. These two files contain
the number of body parts corrected following the two outlier
correction methods for each video in the project. The files will look
similar to the worksheets in <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/misc/Outlier_corrections.xlsx">this Microsoft Excel
file</a>.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some scenarios, SimBA users are confident that the
pose-estimation is perfected, no gross innacuracies are present, and
the user may want to completely skip the use of SimBAs outlier
correction tools. To do this, click on the red
<code class="docutils literal notranslate"><span class="pre">Skip</span> <span class="pre">outlier</span> <span class="pre">correction</span> <span class="pre">(CAUTION)</span></code> button. Clicking on this button
will format your CSV files and make them compatible with subsequent
procedures (see below) without removing any outliers from you
tracking data.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some users are are primarily intrested in using SimBA for
other things than predictive classification. This may include using
SimBA for analyzing animal movements, the animals directionality or
“gaze”, animal velocities / distance travelled, or time spent /
entries made into user-defined regions of interest. If you are
inrested in using these functions in SimBA, you now have all the data
you need and do not need to proceed with the current tutorial.
Instead, head to the <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/ROI_tutorial.md">SimBA ROI
tutorial</a>.</p>
</div>
</section>
<section id="step-5-extract-features">
<h3>Step 5: Extract Features<a class="headerlink" href="#step-5-extract-features" title="Permalink to this heading"></a></h3>
<p>Based on the coordinates of body parts in each frame - and the frame
rate and the pixels per millimeter values - the feature extraction step
calculates a larger set of features used for behavioral classification.
Features are values such as metric distances between body parts, angles,
areas, movement, paths, and their deviations and rank in individual
frames and across rolling windows. This feature set extracted from the
tracking data is what we are going to use to predict behavior BtWGaNP,
using the relationships between features, rather than the pose
estimation data itself.</p>
<p>This set of features will depend on the body-parts tracked during
pose-estimation (which is defined when creating the project). Click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/misc/Feature_description.csv">HERE</a>
for an example list of features when tracking 2 mice and 16 body parts.
Click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/misc/features_user_defined_pose_config.csv">HERE</a>
for an example set of features when using a user-defined body-part
configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some users of SimBA have their own third-party python
feature extraction class, and want to use that class within the SimBA
GUi environment. This may be helful when there is a specific set of
features, not calculated in the SimBA default feature extraction
code, that will be helpful for downstream ML algorithms. To learn how
to use third-party feature extraction classes in SimBA, click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/extractFeatures.md">HERE</a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some users may want to take advantage of a sub-set of the
feature extraction method in SimBA, and use that output data within
their own third-party applications outside of the SimBA environment.
E.g., users may just want to calculate angles, movements and/or sizes
of the animals and save that data in CSV format. To learn how to do
this in SimBA, click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/feature_subsets.md">HERE</a>.</p>
</div>
<ol class="arabic simple">
<li><p>Click on the tab <code class="docutils literal notranslate"><span class="pre">Extract</span> <span class="pre">Features</span></code>, and then the button
<code class="docutils literal notranslate"><span class="pre">Extract</span> <span class="pre">Features</span></code>.</p></li>
</ol>
<a class="reference internal image-reference" href="../_images/extract_features_1.png"><img alt="../_images/extract_features_1.png" class="align-center" src="../_images/extract_features_1.png" style="width: 800px;" /></a>
<p>New CSV files, that contain the feature data and the pose-estimation
data, will be generated and saved in the
<code class="docutils literal notranslate"><span class="pre">project_folder\csv\features_extracted</span></code> directory, with one file for
every DLC tracking file imported to the project. This tab also displays
a second button - <code class="docutils literal notranslate"><span class="pre">Append</span> <span class="pre">ROI</span> <span class="pre">data</span> <span class="pre">to</span> <span class="pre">features</span></code>. We explain the
function of this button in the <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/simba_JJ_branch/docs/ROI_tutorial.md">ROI
tutorial</a>,
and how it can be used to create features from spatial locations/objects
in the videos.</p>
<ol class="arabic simple" start="2">
<li><p>We want to validate the classifier for behavior BtWGaNP on a separate
video, and that video should not be used in the training and testing
steps. In the current Scenario 1, we have generated 20 files
containing features and they are stored in the
<code class="docutils literal notranslate"><span class="pre">project_folder\csv\features_extracted</span></code> folder. To store away one
of these files for later validation, navigate to the
<code class="docutils literal notranslate"><span class="pre">project_folder\csv\features_extracted</span></code> folder, and cut one file
out of the <code class="docutils literal notranslate"><span class="pre">project_folder\csv\features_extracted</span></code> folder, and
paste it somewhere else outside of your <code class="docutils literal notranslate"><span class="pre">project_folder</span></code>. This way,
SimBA won’t see the file in later steps, and it will be omitted from
inclusion for when creating the model. We will later define the
directory path to this file, and try to predict behavior BtWGaNP in
this file from the classifer generated using data in the other 19
data files.</p></li>
</ol>
</section>
<section id="step-6-label-behavior-i-e-create-annotations-for-predictive-classifiers">
<h3>Step 6: Label Behavior (i.e, create annotations for predictive classifiers)<a class="headerlink" href="#step-6-label-behavior-i-e-create-annotations-for-predictive-classifiers" title="Permalink to this heading"></a></h3>
<p>This step is used to label the behaviors in each frames of a video. This
data will be concatenated with the extracted features and used for the
creating behavioral classifier.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>SimBA performs similar functions such as the open-source
JWatcher or commercial Noldus Observer systems, with the exception
that SimBA automates the backend integration of behavioral annotation
with creating predictive classifiers. If you already have such
annotations stored in alternative file formats, like JWatcher or
Noldus Observer, they can be appended directly to the tracking data
and no behavioral annotations needs to be done in SimBA. For example,
the
<a class="reference external" href="http://www.vision.caltech.edu/Video_Datasets/CRIM13/CRIM13/Main.html">CRIM13</a>
dataset was annotated using <a class="reference external" href="https://pdollar.github.io/toolbox/">‘Piotr’s Matlab
Toolbox’</a> and we appended
these annotations to the tracking data using a version of <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/misc/Caltech_2_DLC.py">this
script</a>.</p>
</div>
<p>If you already have annotation videos created with these alternative
tools, or any other behavioral annotator, and would like to use them to
create predictive classifiers, please let us know as we would like to
write scripts that could process these data for SimBA. If you have
created such scripts yourself, please consider contributing them to the
community!</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The behavioral labelling is a critical step. A computer
will only learn what you teach it, so if your annotations are not
correct then the classifiers will fail. <strong>SimBA uses every single
frame</strong>, and therefore the “start” and “stop” frames for a given
behavior are very important. Please take the time to create clear and
well-defined operationalized definitions of the complete behavior,
including start and stop frames. As an example, here are machine
learning operationalized definitions of
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/misc/Operational_definitions_mouse_resident_intruder.pdf">mouse</a>
and
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/misc/Operational_definitions_rat_resident_intruder.pdf">rat</a>
predictive classifiers for aggressive behaviors in resident-intruder
protocols.</p>
</div>
<ol class="arabic simple">
<li><p>Click on the <code class="docutils literal notranslate"><span class="pre">Label</span> <span class="pre">Behavior</span></code> tab in the <code class="docutils literal notranslate"><span class="pre">Load</span> <span class="pre">Project</span></code> window
and you should see the following menu:</p></li>
</ol>
<a class="reference internal image-reference" href="../_images/labelling_1.png"><img alt="../_images/labelling_1.png" class="align-center" src="../_images/labelling_1.png" style="width: 1000px;" /></a>
<p>Use the first button (i) in the image above if you are starting to
create behavior labels for a video from scratch. Use the second button
(ii) to continue annotating a video that you have made behavior labels
for previously in the current project. In the third menu (iii) we have
button-click access to functions that append behavior labels generated
in third-party apps to your SimBA dataset. To read more about how to
append annotations created in third-party applications, click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/third_party_annot.md">HERE</a>.</p>
<ol class="arabic simple">
<li><p>In the current scenario, we want to click on the first button -
<code class="docutils literal notranslate"><span class="pre">Select</span> <span class="pre">video</span> <span class="pre">(create</span> <span class="pre">new</span> <span class="pre">video</span> <span class="pre">annotation)</span></code>. In your project
folder, navigate to the <code class="docutils literal notranslate"><span class="pre">/project_folder/videos</span></code> folder and select
the video file (e.g., mp4 or avi file) representing the video you
wish to annotate.</p></li>
</ol>
<a class="reference internal image-reference" href="../_images/labelling_2.png"><img alt="../_images/labelling_2.png" class="align-center" src="../_images/labelling_2.png" style="width: 1000px;" /></a>
<ol class="arabic simple" start="2">
<li><p>Please click <a class="reference external" href="/docs/labelling_aggression_tutorial.md">here</a> to
learn how to use the behavior annotation interface.</p></li>
<li><p>Once finished, click on <code class="docutils literal notranslate"><span class="pre">Generate/Save</span></code> and it will generate a new
CSV file in <em>project_folder/csv/targets_inserted</em> folder. Repeat this
step for all the files you wish you use to generate the predictive
classifier for behavior BtWGaNP.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you used SimBAs behavior label annotation interface to
create behavior labels for the first half of your video, and now you
want create labels for the second half of a video, then you can do so
by clicking on the second button in the <code class="docutils literal notranslate"><span class="pre">Label</span> <span class="pre">Behavior</span></code> menu
titled <code class="docutils literal notranslate"><span class="pre">Select</span> <span class="pre">video</span> <span class="pre">(Continue</span> <span class="pre">with</span> <span class="pre">existing</span> <span class="pre">video</span> <span class="pre">annotation)</span></code>.
After clicking on this button, follow the same steps as when creating
a new annotations. The difference with clicking on this button will
be that when the SimBA behavior label interface shows up, it will
show up from the last saved frame with your previous annotations for
this file in memory.</p>
</div>
</section>
<section id="step-7-train-machine-model">
<h3>Step 7: Train Machine Model<a class="headerlink" href="#step-7-train-machine-model" title="Permalink to this heading"></a></h3>
<p>This step is used for training new machine models for behavioral
classifications in SimBA. There are a range of machine learning settings
(called hyper-parameters) data sampling method, and evaluation settings,
that influence the performance of the classifiers and how it is
interpreted. We have currated a list of parameters and sampling methods
that can be tweaked and validated. For more technical explanations, see
the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">scikit-learn RandomRandomForestClassifier
API</a>
or join the discussion on our <a class="reference external" href="https://gitter.im/SimBA-Resource/community">Gitter
page</a>.</p>
<p>To facilitate use and make this step as painless as possible, users can
import hyper-parameters, sampling and evaluations setting from
pre-packaged CSV files (SEE BELOW). This may be a lot to look at, so
please read this whole section before starting to build your own
classifiers.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>SimBA allows you to generate predictive classifiers in two
different <em>modes</em>. You can either <strong>(i)</strong> specify a single set of
hyperparameters, and train a single predictive classifiers using that
defined set of hyperparameters. Alternatively, you can <strong>(ii)</strong>
specify many different Hyperparameter settings, and batch train
multiple different models, each using a different set of
Hyperparameters. The second option is relevant for the current
Scenario. For example, here we may want to generate five different
classifiers that all predict the behavior BtWGaNP. We want to
evaluate each one, and proceed to the Experimental data with the
classifier that best captures behavior BtWGaNP in the pilot data and
validation video. Thus, the first section of this part of the
tutorial describes the different Hyperparameter settings, and what
you can do to avoid setting them manually (<em>HINT</em>: you can load them
all from a <em>metadata</em> file at the top of the window), while the
second part of the tutorial section describes how to proceed with
either of the two <em>modes</em> for generating classifiers.</p>
</div>
<section id="train-predictive-classifier-s-settings">
<h4>Train predictive classifier(s): settings<a class="headerlink" href="#train-predictive-classifier-s-settings" title="Permalink to this heading"></a></h4>
<p>Click on <code class="docutils literal notranslate"><span class="pre">SETTINGS</span></code> in the [TRAIN MACHINE MODEL] tab and the
following, slightly indimidating (but I promise: easy to use!) window
will pop up. We will go over the meaning of each of the settings in
turn.</p>
<a class="reference internal image-reference" href="../_images/model_settings_1.png"><img alt="../_images/model_settings_1.png" class="align-center" src="../_images/model_settings_1.png" style="width: 1000px;" /></a>
<p><strong>1. LOAD META DATA</strong>. If you have a CSV file containing hyperparameter
metadata, you can import this file by clicking on <code class="docutils literal notranslate"><span class="pre">Browse</span> <span class="pre">File</span></code> and
then click on <code class="docutils literal notranslate"><span class="pre">Load</span></code>. This will autofill all the Hyperparameter entry
boxes and model evaluation settings. For the Scenario 1, we
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/misc/BtWGaNP_meta.csv">provide</a>
a Metadata file that will populate the Hyperparameter entry boxes and
evaluation settings with some default values. Please save this file to
disk and load it. If you downloaded SimBA through our github page, the
Metadata file should be in the <em>simba/misc</em> directory. Alternatively,
there is an up-to-date meta
file<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/misc/meta_data_2023.csv">HERE</a>
which covers additional newer settings recently included in SimBA
(including class weights ans shapley values).</p>
<p><strong>2. MACHINE MODEL ALGORITHM</strong>. Choose a machine model algorithm to use
from the drop down menu: <code class="docutils literal notranslate"><span class="pre">RF</span></code> ,<code class="docutils literal notranslate"><span class="pre">GBC</span></code>,<code class="docutils literal notranslate"><span class="pre">Xboost</span></code>. For this
scenario, we will choose RF (<em>Note:</em>: GBC and XGBoost options are still
under development).</p>
<p><strong>3. BEHAVIOR</strong>. Use the drop-down menu to select the behavioral
classifier you wish to define the hyper-parameters for. In this
Scenario, only one <em>BEHAVIOR</em> should be seen in the drop-down menu
(BtWGaNP). If you are generating multiple classifiers, they should all
be seen in the drop-down menu.</p>
<p><strong>4. RANDOM FOREST ESTIMATORS</strong>. The number of decision trees we should
use in our classifier. Typically a value between 100-2000. In most
instances, a higher value won’t hurt you. If you have a large dataset
and are seeing <code class="docutils literal notranslate"><span class="pre">MemoryError</span></code> while building the classifier, then try
to decrease the number of estimators.</p>
<p><strong>5. MAX FEATURES</strong>. Number of features to consider when looking for the
best split. Select <code class="docutils literal notranslate"><span class="pre">sqrt</span></code> from the dropdown to use the square root of
the total number of features in your dataset when evaluating a split.
Select <code class="docutils literal notranslate"><span class="pre">log</span></code> from the dropdown to use the log of the total number of
features in your dataset when evaluating a split. Select None from the
dropdown to use all of the features in your dataset when evaluating a
split.</p>
<p><strong>6. CRITERION</strong>. Select the metric used to measure the quality of each
split (<em>gini</em> or <em>entropy</em>).</p>
<p><strong>7. TEST SIZE</strong>. Select the ratio of your data that should be used to
test your model. For example, selecting <code class="docutils literal notranslate"><span class="pre">0.2</span></code> from the drop-down will
results in the model beeing trained on 80% of your data, and tested on
20% of your data.</p>
<p><strong>8. TRAIN TEST SPLIT TYPE</strong>. Select how you want to sample annotated
frames for training and testing data-sets. If you select <strong>FRAMES</strong>,
then randomly selected frames will be assigned to the training and
testing datasets. Although such a random selection can create accurate
classifiers, the performance metric may be inflated due to time-series
information <em>leakage</em> between the training and test-sets. For more
information, see
<a class="reference external" href="https://towardsdatascience.com/avoiding-data-leakage-in-timeseries-101-25ea13fcb15f">HERE</a>.
To decrease the potential of such leakage, select <strong>BOUTS</strong> from the
<code class="docutils literal notranslate"><span class="pre">Train-Test</span> <span class="pre">Split</span> <span class="pre">Type</span></code> drop-down menu. If you select <strong>BOUTS</strong>, then
none of the annotated frames in the classifier training set will come
from the same annotated behavior bout as in the classifier testing set.
We encurage users to <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Scenario1.md#critical-validation-step-before-running-machine-model-on-new-data">validate their
classifiers</a>
outside of classifier training before proceeding. See the below image
for an illustration of using <strong>FRAMES</strong> vs <strong>BOUTS</strong> Train-Test Split
Types:</p>
<a class="reference internal image-reference" href="../_images/bout_vs_frames_split1.png"><img alt="../_images/bout_vs_frames_split1.png" class="align-center" src="../_images/bout_vs_frames_split1.png" style="width: 1000px;" /></a>
<p><strong>9. MINIMUM SAMPLE LEAF</strong>. The minimum number of samples required to be
a leaf node (e.g., <em>1</em>, or increase this value to <a class="reference external" href="https://elitedatascience.com/overfitting-in-machine-learning">prevent
over-fitting</a>).</p>
<p><strong>10. UNDER-SAMPLE SETTING</strong>. <code class="docutils literal notranslate"><span class="pre">random</span> <span class="pre">undersample</span></code> or <code class="docutils literal notranslate"><span class="pre">None</span></code>. If
<code class="docutils literal notranslate"><span class="pre">random</span> <span class="pre">undersample</span></code>, a random sample of the majority class
annotations (usually behavior absent) will be used in the training set.
The size of this sample will be taken as a ratio of the minority class
(usually behavior present) and the ratio is specified in the
<code class="docutils literal notranslate"><span class="pre">under-sample</span> <span class="pre">ratio</span></code> box (see below). For more information, click
<a class="reference external" href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html">HERE</a>
or
<a class="reference external" href="https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/">HERE</a>.
This setting address issues that arise from “imbalanced” data sets,
where the behavior that is predicted is sparse. <strong>Note:</strong> <em>Class</em> means
the classification that a video frame belongs to. In this Scenario it is
either (i) Not BtWGaNP, or (ii) BtWGaNP. The majority class is the class
of frames that contains the most examples, which - most certaily - in
any use case of SimBA, will be <em>Not BtWGaNP</em>. Conversely, the minority
class is the class of frames that contains the least examples, which
will be <em>BtWGaNP</em>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If your behavior is sparse (say you have annotated the behavior
to be present in 1-10% of the total number of frames), see if you can
improve your classifier by setting the <code class="docutils literal notranslate"><span class="pre">UNDER-SAMPLE</span> <span class="pre">SETTING</span></code> to
<code class="docutils literal notranslate"><span class="pre">random</span> <span class="pre">undersample</span></code> and the <code class="docutils literal notranslate"><span class="pre">under-sample</span> <span class="pre">ratio</span></code> to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
</div>
<p><strong>11. UNDER-SAMPLE RATIO</strong>. The ratio of behavior-absent annotation to
behavior-present annotation to use in the training set. For example, if
set to <code class="docutils literal notranslate"><span class="pre">1.0</span></code>, an equal number of behavior-present and behavior-absent
annotated frames will be used for training. If set to <code class="docutils literal notranslate"><span class="pre">2.0</span></code>, twice as
many behavior-absent annotatations than behavior-present annotations
will be used for training. If set to <code class="docutils literal notranslate"><span class="pre">0.5</span></code>, twice as many
behavior-present annotatations than behavior-absent annotations will be
used for training.</p>
<p><strong>12. OVER SAMPLE SETTING</strong>. <code class="docutils literal notranslate"><span class="pre">SMOTE</span></code>, <code class="docutils literal notranslate"><span class="pre">SMOTEEN</span></code> or <code class="docutils literal notranslate"><span class="pre">None</span></code>. If
“SMOTE” or “SMOTEEN”, synthetic data will be generated in the minority
class based on k-mean distances in order to balance the two classes. See
the imblearn API for details on
<a class="reference external" href="https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html">SMOTE</a>
and
<a class="reference external" href="https://imbalanced-learn.org/stable/references/generated/imblearn.combine.SMOTEENN.html">SMOTEEN</a>.</p>
<p><strong>13. OVER SAMPLE RATIO</strong>. The desired ratio of the number of samples in
the minority class over the number of samples in the majority class
after over synthetic sampling. For example, if <code class="docutils literal notranslate"><span class="pre">1.0</span></code>, N number of
synthetic behavior-present annotations will be created so the total
count of behavior-present (synthetic and real) equals the total count of
behavior-absent annotated frames.</p>
<p><strong>14. CLASS WEIGHT SETTINGS</strong>. Although <code class="docutils literal notranslate"><span class="pre">random</span> <span class="pre">undersampling</span></code>
discussed in point <code class="docutils literal notranslate"><span class="pre">9</span></code> above can produce accurate classifiers, it has
a major drawback. When performing undersampling we basically chuck out a
bunch of data from the minority class. Instead, we may want to keep
<strong>all</strong> the data, but value accurate observations of the minority class
equally to the observations in the majority class by assigning weights
to the different classes. To do this, select
<code class="docutils literal notranslate"><span class="pre">`balanced</span></code> &lt;<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>&gt;`__
or
<code class="docutils literal notranslate"><span class="pre">`balanced_subsample</span></code> &lt;<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>&gt;`__
in the <code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">weight</span> <span class="pre">settings</span></code> dropdown.</p>
<p>You can also assign your own weights to the two different classes of
observations (behavior-present versus behavior-absent) by selecting the
<code class="docutils literal notranslate"><span class="pre">custom</span></code> option in the <code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">weight</span> <span class="pre">settings</span></code> dropdown. For
example, setting behavior PRESENT to <code class="docutils literal notranslate"><span class="pre">2</span></code>, and behavior ABSENT to <code class="docutils literal notranslate"><span class="pre">1</span></code>
will lead the classifier to attribute twice the importance to behavior
present annotatons over behavior absent annotations.</p>
<a class="reference internal image-reference" href="../_images/class_weights.png"><img alt="../_images/class_weights.png" class="align-center" src="../_images/class_weights.png" style="width: 1000px;" /></a>
<p><strong>15. CREATE MODEL META DATA FILE</strong>. Creates a CSV file listing the
hyper-parameter settings used when creating the classifier. The
generated meta file can be used to create further models by importing it
in the <code class="docutils literal notranslate"><span class="pre">LOAD</span> <span class="pre">SETTINGS</span></code> menu (see <strong>Step 1</strong> above).</p>
<p><strong>16. CREATE Example Decision Tree - graphviz</strong>. Saves a visualization
of a random decision tree in PDF and .DOT formats. Requires
<a class="reference external" href="https://graphviz.gitlab.io/">graphviz</a>. For more information on this
visualization, click
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html">here</a>.
For information on how to install on Windows, click
<a class="reference external" href="https://bobswift.atlassian.net/wiki/spaces/GVIZ/pages/20971549/How+to+install+Graphviz+software">here</a>.
For an example of a graphviz decision tree visualization generated
through SimBA, click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/images/BtWGaNP_tree.pdf">here</a>.
<em>Note:</em> The trees can be very large depending on the Hyperparameter
settings. Rather than using a dedicated PDF viewer, try opening the
generated PDF by dragging it into a web browser to get a better view.</p>
<p><strong>17. Fancy Example Decision Tree - dtreeviz</strong>. Saves a nice looking
visualization of a random decision tree in SVG format. Requires
<a class="reference external" href="https://github.com/parrt/dtreeviz">dtreeviz</a>. For an example of a
dtreeviz decision tree visualization generated through SimBA, click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/images/dtreeviz_SimBA.png">here</a>.
<em>Note:</em> These SVG example decision trees are very large. To be able to
view them on standard computers, SimBA limits the depth of the example
tree to 3 levels.</p>
<p><strong>18. Create Classification Report</strong>. Saves a classification report
truth table in PNG format displaying precision, recall, f1, and support
values. For more information, click
<a class="reference external" href="http://www.scikit-yb.org/zh/latest/api/classifier/classification_report.html">here</a>.
For an example of a classification report generated through SimBA, click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/images/BtWGaNP_classificationReport.png">here</a>.</p>
<p><strong>19. Create Features Importance Bar Graph</strong>. Creates a CSV file that
lists the importance’s <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html">(gini
importances)</a>.
For an example of a feature importance list generated through SimBA,
click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/images/BtWGaNP_feature_importance_log.csv">here</a>.
Also creates a bar chart depecting the top N features features in the
classifier. For an example of a bar chart depicting the top <em>N</em> features
generated through SimBA, click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/images/BtWGaNP_feature_bars.png">here</a>
<em>Note:</em> Although <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html">gini
importances</a>
gives an indication of the most important features for predicting the
behavior, there are known <a class="reference external" href="https://explained.ai/rf-importance/">flaws when assessing well correlated
features</a>. As SimBA uses a very
large set of well correlated features to enable flexible usage, consider
evaluating features through permutation importance calculatations
instead or shapley values (see below).</p>
<p><strong>20. # Features</strong>. The number of top N features to plot in the
<code class="docutils literal notranslate"><span class="pre">Create</span> <span class="pre">Features</span> <span class="pre">Importance</span> <span class="pre">Bar</span> <span class="pre">Graph</span></code>.</p>
<p><strong>21. Compute Feature Permutation Importances</strong>: Creates a CSV file
listing the importance’s (permutation importance’s) of all features for
the classifier. For more details, please click
<a class="reference external" href="https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html">here</a>).
Note that calculating permutation importance’s is computationally
expensive and takes a long time. For an example CSV file that list
featue permutation importances, click [here]
(<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/images/BtWGaNP_prediction_permutations_importances.csv">https://github.com/sgoldenlab/simba/blob/master/images/BtWGaNP_prediction_permutations_importances.csv</a>).</p>
<p><strong>22. Create Learning Curves</strong>: Creates a CSV file listing the f1 score
at different test data sizes. For more details on learning curves,
please click
<a class="reference external" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html">here</a>).
For more information on the f1 performance score, click
<a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">here</a>. The learning curve is
useful for estimating the benefit of annotating further data. For an
example CSV file of the learning curve generated through SimBA, click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/images/BtWGaNP_prediction_learning_curve.csv">here</a>.</p>
<p><strong>23. Learning Curve Shuffle K Splits</strong>: Number of cross validations
applied at each test data size in the learning curve.</p>
<p><strong>24. Learning Curve shuffle Data Splits</strong>: Number of test data sizes in
the learning curve.</p>
<p><strong>25. Create Precision Recall Curves</strong>: Creates a CSV file listing
precision at different recall values. This is useful for titration of
the false positive vs. false negative classifications of the models by
manipulating the <code class="docutils literal notranslate"><span class="pre">Discrimination</span> <span class="pre">threshold</span></code> (see below). For an
example CSV file of of a precision-recall curve generated through SimBA,
click
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/misc/BtWGaNP_prediction_precision_recall.csv">here</a>.
This may be informative if your aiming to titrate your classifier to (i)
predict all occurances of behavior BtWGaNP, and can live with a few
false-positive classifications, or (ii) conversely, you want all
predictions of behavior BtWGaNP to be accurate, and can live with a few
false-negative classifications, or (iii) you are aiming for a balance
between false-negative and false-positive classifications.</p>
<p><strong>26. Calculate SHAP scores</strong>: Creates a CSV file listing the
contribution of each individual feature to the classification
probability of each frame. For more information, see the <a class="reference external" href="https://github.com/sgoldenlab/simba/edit/master/docs/SHAP.md">SimBA SHAP
tutorial</a>
and the <a class="reference external" href="https://github.com/slundberg/shap">SHAP GitHub repository</a>.
SHAP calculations are an computationally expensive process, so we most
likely need to take a smaller random subset of our video frames, and
calculate SHAP scores for this random subset.</p>
<p><strong>27 # target present</strong>: The number of frames (integer - e.g., <code class="docutils literal notranslate"><span class="pre">100</span></code>)
with the behavioral target <strong>present</strong> (with the behavioral target being
the behavior selected in the <strong>Behavior</strong> drop-down menu in Step 3
above) to calculate SHAP values for.</p>
<p><strong>28 # target absent</strong>: The number of frames (integer - e.g., <code class="docutils literal notranslate"><span class="pre">100</span></code>)
with the behavioral target <strong>absent</strong> (with the behavioral target being
the behavior selected in the <strong>Behavior</strong> drop-down menu in Step 3
above) to calculate SHAP values for.</p>
<ul class="simple">
<li><p><strong>ADDITIONAL SETTINGS NOT YET DOCUMENTED ABOVE</strong>:</p></li>
</ul>
<ol class="lowerroman simple">
<li><p><strong>SHAP save cadence</strong> SHAP calculations are an computationally
expensive, and calculating SHAP values on larger datasets come with
long run-times. As a safety pre-caution, you can intermittently
save your SHAP values to disk. For example, if you want to save
your data every 100 frames, set this value to a 100. If you want to
save the data only when all SHAP calculations are complete, then
select <code class="docutils literal notranslate"><span class="pre">ALL</span> <span class="pre">FRAMES</span></code> from the <code class="docutils literal notranslate"><span class="pre">SHAP</span> <span class="pre">save</span> <span class="pre">cadence</span></code> dropdown menu.</p></li>
<li><p><strong>Calculate partial dependencies</strong> Partial dependencies is another
<em>global explainability</em> method, that helps you understand the
relationship between individual feature values and classification
probabilities. For an accessable description of partial
dependencies, see <a class="reference external" href="https://towardsdatascience.com/partial-dependence-plots-with-scikit-learn-966ace4864fc">this blog
post</a>.
Ticking this box will calculate partial dependencies for ecery
feature in your data set, and one CSV file will be saved for every
feature. For an example of this CSV file, click [HERE]. BEWARE:
Calculating parial dependencies come with long run-times.</p></li>
</ol>
</section>
<section id="train-predictive-classifier-s-start-the-machine-training">
<h4>Train predictive classifier(s): start the machine training<a class="headerlink" href="#train-predictive-classifier-s-start-the-machine-training" title="Permalink to this heading"></a></h4>
<p>Once all the entry boxes have been filled in with the desired
Hyperparameters and Model Evaluation Settings, the user can either click
on <code class="docutils literal notranslate"><span class="pre">Save</span> <span class="pre">settings</span> <span class="pre">into</span> <span class="pre">global</span> <span class="pre">environment</span></code> or
<code class="docutils literal notranslate"><span class="pre">Save</span> <span class="pre">settings</span> <span class="pre">for</span> <span class="pre">specific</span> <span class="pre">model</span></code>. If you click on
<code class="docutils literal notranslate"><span class="pre">Save</span> <span class="pre">settings</span> <span class="pre">into</span> <span class="pre">global</span> <span class="pre">environment</span></code>, the settings will be saved in
to your <em>project_config.ini</em> file located in your <code class="docutils literal notranslate"><span class="pre">project_folder</span></code>.
These settings can subsequently be retreived and executed to generate a
predictive classifier (<strong>Mode 1</strong>).</p>
<p>However, if you click on <code class="docutils literal notranslate"><span class="pre">Save</span> <span class="pre">settings</span> <span class="pre">for</span> <span class="pre">specific</span> <span class="pre">model</span></code>, a config
file will be generated in the background (in the
<code class="docutils literal notranslate"><span class="pre">/project_folder/configs</span></code> folder) that contains the defined settings.
To generate 5 different config files with different settings, for
example, simply update the Hyperparameters and Model Evaluation Settings
five different times, representing the five different predictive
classifiers you want to generate, and after each time you have updated
the values, click on <code class="docutils literal notranslate"><span class="pre">Save</span> <span class="pre">settings</span> <span class="pre">for</span> <span class="pre">specific</span> <span class="pre">model</span></code> (<strong>Mode 2</strong>).
See below for more information.</p>
<p><strong>(i) Mode 1</strong>: To save the settings into the global environment, and
generate a single predictive classifier using these settings, click on
<code class="docutils literal notranslate"><span class="pre">Save</span> <span class="pre">settings</span> <span class="pre">into</span> <span class="pre">global</span> <span class="pre">environment</span></code>, and then <strong>exit the machine
model settings window by closing it</strong>. Next, click on the blue button:
<code class="docutils literal notranslate"><span class="pre">Train</span> <span class="pre">single</span> <span class="pre">model</span> <span class="pre">from</span> <span class="pre">global</span> <span class="pre">environment</span></code>. While the model is
training, text is printed in the main SimBA window indicating its
progress. A message saying that training is complete will also be
printed in the terminal window, together with information on where you
can find the generated files. The model, in <em>.sav file format</em>, will be
saved in the <code class="docutils literal notranslate"><span class="pre">project_folder\models\generated_models</span></code> folder. If you
have chosen to generate model evaluation files, then they will be be
saved in the
<code class="docutils literal notranslate"><span class="pre">project_folder\models\generated_models\model_evaluations</span></code> folder.</p>
<p><strong>(ii) Mode 2</strong>: Alternatively, click on the
<code class="docutils literal notranslate"><span class="pre">Save</span> <span class="pre">settings</span> <span class="pre">for</span> <span class="pre">specific</span> <span class="pre">model</span></code> button to save the settings for one
model. To generate multiple models - for multiple different
Hyperparameters used to predict behavior BtWGaNP - redefine the Machine
model settings and click on <code class="docutils literal notranslate"><span class="pre">Save</span> <span class="pre">settings</span> <span class="pre">for</span> <span class="pre">specific</span> <span class="pre">model</span></code> again.
Each time the <code class="docutils literal notranslate"><span class="pre">Save</span> <span class="pre">settings</span> <span class="pre">for</span> <span class="pre">specific</span> <span class="pre">model</span></code> is clicked, a new
config file is generated in the <em>/project_folder/configs</em> folder. Next,
<strong>exit the machine model settings window by closing it</strong> and click on
the <em>green</em> button:
<code class="docutils literal notranslate"><span class="pre">Train</span> <span class="pre">multiple</span> <span class="pre">models,</span> <span class="pre">one</span> <span class="pre">for</span> <span class="pre">each</span> <span class="pre">saved</span> <span class="pre">setting</span></code>. This will
generate one model for each of the config files in the
<em>/project_folder/configs</em> folder. The models, in <em>.sav file format</em>,
will be saved in the <code class="docutils literal notranslate"><span class="pre">project_folder\models\validations\model_files</span></code>
folder. Model evaluation files will be saved in the
<code class="docutils literal notranslate"><span class="pre">project_folder\models\validations\model_evaluations</span></code> folder. Model
meta files, will be saved in the
<code class="docutils literal notranslate"><span class="pre">project_folder\models\validations\meta_data</span></code> directory.</p>
</section>
</section>
<section id="step-8-evaluating-the-model-on-new-out-of-sample-data">
<h3>Step 8. Evaluating the model on new (out-of-sample) data.<a class="headerlink" href="#step-8-evaluating-the-model-on-new-out-of-sample-data" title="Permalink to this heading"></a></h3>
<p>If you have chosen to generate classification reports or other metrics
of classifier performance, it is worth studying them to ensure that the
model(s) performance is acceptable. However, a classifiers performance
is perhaps most readily validated by visualizing its predictions and
prediction probabilities on a new video, which have not been used for
training or testing. This step is critical for (i) visualizing and
choosing the ideal classification probability thresholds which captures
all of your BtWGaNP behaviors (for more information on what
classification threshold is - see Step 4 below), and (i) visual
confirmation that model performance is sufficent for running it on
experimental data.</p>
<p>You can validate each model <em>(saved in SAV format)</em> file. This should be
done in a “gold-standard” video that has been fully manually annotated
for your behavior of interest, but has not been included in the training
dataset. If you followed the tutorial, you may remember that we stored
away one CSV file away in a safe place earlier, a <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Scenario1.md#step-5-extract-features">file which we had
extracted the
features</a>
for but we did not use this file for training or testing of the
classifier. Now is the time to use this file.</p>
<p>In this validation step the user specifies the path to a previously
created model in SAV file format, and the path to a CSV file <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Scenario1.md#step-5-extract-features">that
contain the features extracted from a video (Step 5
above)</a>.
The first process will run the predictions on the video, and visualize
the probabilities in a user-interactable line chart, that together with
a user-interactable visualization of the video can be used to gauge the
ideal discrimination threshold. The second process will create a video
with the predictions overlaid together with a gantt plot showing
predicted behavioral bouts. Click
<a class="reference external" href="https://youtu.be/UOLSj7DGKRo">here</a> for an expected output
validation video for predicting the behavior <em>copulation</em>.</p>
<p>This process allows you to rapidly access the results of the
Hyperparameters you have selected on a “gold-standard” behavioral video.
If the predictions are not good, you can go back to tweak the
appropriate parameters without first running through numerous other
videos or adding and/or refining your annotations.</p>
<p>In this step, we will (i) run the classifier on new data, (ii)
interactively inspect suitable discrimination thresholds, and (iii)
create a video with the predictions overlaid ontop of the new data
together with a gantt plot showing predicted behavioral bouts. Click
<a class="reference external" href="https://youtu.be/UOLSj7DGKRo">HERE</a> for an expected validation
video. For this, navigate to the [Run machine model] tab and `VALIDATE
MODEL ON SINGLE VIDEO menu:</p>
<a class="reference internal image-reference" href="../_images/validate_single_video_1.png"><img alt="../_images/validate_single_video_1.png" class="align-center" src="../_images/validate_single_video_1.png" style="width: 1000px;" /></a>
<p><strong>(1).</strong> In <code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">FEATURE</span> <span class="pre">DATA</span> <span class="pre">FILE</span></code>, select the path to a path to a
file containing features in the
<code class="docutils literal notranslate"><span class="pre">project_folder/csv/features_extracted</span></code> directory created as describes
in <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Scenario1.md#step-5-extract-features">STEP
5</a>
above. Note: Make sure this file is <strong>not</strong> a file where the behavior
has been labeleld and used to create the classifier you are evaluating.</p>
<p><strong>(2).</strong> In <code class="docutils literal notranslate"><span class="pre">SELECT</span> <span class="pre">MODEL</span> <span class="pre">FILE</span></code>, select the path to a path a
classifier. In SimBA, classifier are saved in <code class="docutils literal notranslate"><span class="pre">.sav</span></code> file format and
typically live in the <code class="docutils literal notranslate"><span class="pre">model</span></code> sub-directories within your SimBA
project.</p>
<p><strong>(3).</strong> To run the model selected in step 2 on the feature data
selected in step 1, click the <code class="docutils literal notranslate"><span class="pre">RUN</span></code> button. In the background, SimBA
will generate a behavior probability score for each of the frames in the
data and store it in the <code class="docutils literal notranslate"><span class="pre">project_folder/csv/validation</span></code> directory of
your SimBA project.</p>
<p><strong>(4).</strong> Next, we want to interactively inspect the prediction
probabilities of each frame and view them alongside the video. We do
this to try an discern a prediction probability demarcation point where
the model reliably splits behavior from non-behavior frames. In other
words, we determine how sure the model has to be that a behavior occurs
on a frame for it to classify a behavior to occur in a frame. To do
this, click the <code class="docutils literal notranslate"><span class="pre">INTERACTIVE</span> <span class="pre">PROBABILITY</span> <span class="pre">PLOT</span></code> button.</p>
<p><a class="reference external" href="https://user-images.githubusercontent.com/34761092/229304497-8f0f4532-e613-4f96-bcda-dededca39dc6.mp4">https://user-images.githubusercontent.com/34761092/229304497-8f0f4532-e613-4f96-bcda-dededca39dc6.mp4</a></p>
<p>In the left window in the example above, you can see the video of the
analyzed file. Similar to the <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/label_behavior.md">annotation
interface</a>,
you can you the buttons to jump forwards and backwards between frames.
Some keyboard shortcuts are showed on the right. There is also a button
named <code class="docutils literal notranslate"><span class="pre">SHOW</span> <span class="pre">HIGHEST</span> <span class="pre">PROBABILITY</span> <span class="pre">FRAME</span></code>. Clicking on this button will
show you the frame with the highest behavioral classification
probability.</p>
<p>In the right window we see the frame number on the x-axis and the
classification probability on the y-axis. Clicking on any frame in the
graph will display the associated video frame in the left window. The
frame number, and the classification probability of the frame, is shown
in the graph title. We look at the graph and determine a suitable
behavioral probability threshold (the y-axis in the right graph) that
separetes the non-behavior from the behavior frames.</p>
<p><strong>(5).</strong> Once we have decided on the probability threshold, we fill this
value into the <code class="docutils literal notranslate"><span class="pre">DISCRIMINATION</span> <span class="pre">THRESHOLD</span> <span class="pre">(0.00-1.0):</span></code> entry box. For
example, if set to 0.50, then all frames with a probability of
containing the behavior of 0.5 or above will be classified as containing
the behavior. For further information on classification theshold, click
<a class="reference external" href="https://www.scikit-yb.org/en/latest/api/classifier/threshold.html">here</a>.
In this Scenario. Go ahead and enter the classification threshold
identified in the previous Steps.</p>
<p><strong>(6).</strong> We can also set a <code class="docutils literal notranslate"><span class="pre">MINIMUM</span> <span class="pre">BOUT</span> <span class="pre">LENGTH</span> <span class="pre">(MS)</span></code> criterion. This
value represents the minimum length of a classified behavioral bout.
<strong>Example</strong>: The random forest makes the following predictions for
behavior BtWGaNP over 9 consecutive frames in a 50 fps video:
1,1,1,1,0,1,1,1,1. This would mean, if we don’t have a minimum bout
length, that the animals enganged in behavior BtWGaNP for 80ms (4
frames), took a break for 20ms (1 frame), then again enganged in
behavior BtWGaNP for another 80ms (4 frames). You may want to classify
this as a single 180ms behavior BtWGaNP bout, rather than two separate
80ms BtWGaNP bouts. If the minimum behavior bout length is set to 20,
any interruption in the behavior that is 20ms or shorter will be removed
and the example behavioral sequence above will be re-classified as:
1,1,1,1,1,1,1,1,1 - and instead classified as a single 180ms BtWGaNP
bout.</p>
<p><strong>(7).</strong> Next we want to go ahead and create a validation video and we
click on <code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">VALIDATION</span> <span class="pre">VIDEO</span></code> and the following pop-up should be
shown which gives user controls how the video is created. If you want to
use the deafult parameters, just go ahead and click <code class="docutils literal notranslate"><span class="pre">RUN</span></code>.</p>
<a class="reference internal image-reference" href="../_images/validate_single_video_2.png"><img alt="../_images/validate_single_video_2.png" class="align-center" src="../_images/validate_single_video_2.png" style="width: 1000px;" /></a>
<ul class="simple">
<li><p>If you want SimBA to try to autompute the appropriate font sizes
etc., keep the <code class="docutils literal notranslate"><span class="pre">AUTO</span> <span class="pre">COMPUTE</span> <span class="pre">STYLES</span></code> checked. Otherwise, un-check
this box and fill in try out your own values.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TRACKING</span> <span class="pre">OPTIONS</span></code>: Choose if you want to display the
pose-estimation body-part locations and/or the animal names in the
validation video.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MULTI-PROCESSING</span> <span class="pre">SETTINGS</span></code>: Creating videos can be computationally
costly, and creating many, long, videos can come with unacceptable
run-times. We can solve this using multiprocessing over multiple
cores on your computer. To use multi-processing, tick the
Multiprocess videos (faster) checkbox. Once ticked, the CPU cores
dropdown becomes enabled. This dropdown contains values between 2 and
the number of cores available on your computer, with fancier
computers having higher CPU counts. In this dropdown, select the
number of cores you want to use to create your video. SimBA will
create a snippet of video on each core, and then jopin them together
to a single video.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">GANTT</span> <span class="pre">SETTINGS</span></code>: If you select <code class="docutils literal notranslate"><span class="pre">Gantt</span> <span class="pre">chart:</span> <span class="pre">video</span></code> or
<code class="docutils literal notranslate"><span class="pre">Gantt</span> <span class="pre">chart:</span> <span class="pre">final</span> <span class="pre">frame</span> <span class="pre">only</span> <span class="pre">(slightly</span> <span class="pre">faster)</span></code> in the
<code class="docutils literal notranslate"><span class="pre">Create</span> <span class="pre">Gantt</span> <span class="pre">plot</span></code> drop-down menu, SimBA will create a validation
video with an appended Gantt chart plot (see the final gif image in
this tutorial below for an example). Creating Gantt charts take
longer, and we suggest selecting <code class="docutils literal notranslate"><span class="pre">None</span></code> in the
<code class="docutils literal notranslate"><span class="pre">Create</span> <span class="pre">Gantt</span> <span class="pre">plot</span></code> drop-down menu unles syou use multi-processing.</p></li>
</ul>
<p><strong>(8).</strong> Click the <code class="docutils literal notranslate"><span class="pre">RUN</span></code> button. You can follow the progress in the
main operating system terminal. Once complete, you should see a video
file representing the analyzed file inside the
<code class="docutils literal notranslate"><span class="pre">project_folder/frames/output/validation</span></code> directory.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>SimBA does offer a further form of
<code class="docutils literal notranslate"><span class="pre">`classifier</span> <span class="pre">validation</span></code> visualization
tool &lt;<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/classifier_validation.md">https://github.com/sgoldenlab/simba/blob/master/docs/classifier_validation.md</a>&gt;`__.
This further tool was designed to visualize the possible presence and
extent of false-positives classifications. The tool displays the
bouts classified as containing the target behavior, with a printed
overlay displaying the total number of frames contained each bout,
the frames conatined within the bout, and the probability that each
specific bout contains the behavior. For more infomration on the
<code class="docutils literal notranslate"><span class="pre">Classifier</span> <span class="pre">Validation</span></code> tool, see the tutoral on the
<a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/classifier_validation.md">Post-classification Validation (detecting
false-positives)</a>.</p>
</div>
<p>Congrats! You have now generated RF models that can be run on new,
experimental data, and we hope those models are doing a good job at
recognizing the behaviors of intrest. Proceed to <a class="reference external" href="https://github.com/sgoldenlab/simba/edit/master/docs/Scenario2.md">Scenario
2</a>
to use the BtWGaNP classifier to evaluate new experimental data. If you
already have further data that has been processed (outliers are
corrected, and the featureshave been calculated), and you now want to
run the classifier on this data, proceed to <a class="reference external" href="https://github.com/sgoldenlab/simba/blob/master/docs/Scenario2.md#part-3-run-the-classifier-on-new-data">Scenario 2 - Part 3: Run
the classifier on new
data</a>.</p>
<p>Author <a class="reference external" href="https://github.com/sronilsson">Simon N</a>, <a class="reference external" href="https://github.com/inoejj">JJ
Choong</a></p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../walkthroughs.html" class="btn btn-neutral float-left" title="Walkthroughs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="scenario_2.html" class="btn btn-neutral float-right" title="Scenario 2 walkthrough" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, sronilsson.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>